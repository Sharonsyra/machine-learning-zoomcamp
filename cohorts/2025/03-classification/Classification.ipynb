{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd66090-49b5-4b5c-9d01-f99b192f7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a489ad97-5ba8-41d0-b99c-5c225db7a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "\n",
    "\n",
    "data = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901f6d7f-4146-4acb-82c5-9a6214122021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-16 09:29:13--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80876 (79K) [text/plain]\n",
      "Saving to: ‘data-week-3.csv’\n",
      "\n",
      "data-week-3.csv     100%[===================>]  78.98K   174KB/s    in 0.5s    \n",
      "\n",
      "2025-10-16 09:29:14 (174 KB/s) - ‘data-week-3.csv’ saved [80876/80876]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget $data -O data-week-3.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd25784e-7a24-4e9d-9b4b-04222e6eaed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
       "0      paid_ads         NaN                         1        79450.0   \n",
       "1  social_media      retail                         1        46992.0   \n",
       "2        events  healthcare                         5        78796.0   \n",
       "3      paid_ads      retail                         2        83843.0   \n",
       "4      referral   education                         3        85012.0   \n",
       "\n",
       "  employment_status       location  interaction_count  lead_score  converted  \n",
       "0        unemployed  south_america                  4        0.94          1  \n",
       "1          employed  south_america                  1        0.80          0  \n",
       "2        unemployed      australia                  3        0.69          1  \n",
       "3               NaN      australia                  1        0.87          0  \n",
       "4     self_employed         europe                  3        0.62          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data-week-3.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aeec164-ee27-4597-9b3f-c70a99539a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source                 128\n",
       "industry                    134\n",
       "number_of_courses_viewed      0\n",
       "annual_income               181\n",
       "employment_status           100\n",
       "location                     63\n",
       "interaction_count             0\n",
       "lead_score                    0\n",
       "converted                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366c370c-3098-40e7-b8b4-b2737c184f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source                  object\n",
       "industry                     object\n",
       "number_of_courses_viewed      int64\n",
       "annual_income               float64\n",
       "employment_status            object\n",
       "location                     object\n",
       "interaction_count             int64\n",
       "lead_score                  float64\n",
       "converted                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef26cebf-0112-4a44-b1e4-948d4ac6e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e55dd849-0f50-4dca-a87d-83348f2e5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_columns] = df[categorical_columns].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1303d54-c147-4730-b2ef-e76d92b543e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source          0\n",
       "industry             0\n",
       "employment_status    0\n",
       "location             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[categorical_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba841aba-cfa1-48c8-b233-e8676c4d7dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_of_courses_viewed      0\n",
       "annual_income               181\n",
       "interaction_count             0\n",
       "lead_score                    0\n",
       "converted                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[numerical_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16196272-23c0-4e5e-b652-d744198aa210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number_of_courses_viewed',\n",
       " 'annual_income',\n",
       " 'interaction_count',\n",
       " 'lead_score',\n",
       " 'converted']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "df.columns\n",
    "\n",
    "numerical_columns = list(df.dtypes[df.dtypes != 'object'].index)\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d46a2f47-c5bc-43b3-9b62-07bb135db863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lead_source', 'industry', 'employment_status', 'location']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8264916-a3fa-46f6-a52a-ed7bab967cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       79450.0\n",
       "1       46992.0\n",
       "2       78796.0\n",
       "3       83843.0\n",
       "4       85012.0\n",
       "         ...   \n",
       "1457        NaN\n",
       "1458    65259.0\n",
       "1459    45688.0\n",
       "1460    71016.0\n",
       "1461    92855.0\n",
       "Name: annual_income, Length: 1462, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.annual_income = pd.to_numeric(df.annual_income, errors='coerce')\n",
    "df.annual_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16eed488-0437-4990-97b6-393d73949d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       79450.0\n",
       "1       46992.0\n",
       "2       78796.0\n",
       "3       83843.0\n",
       "4       85012.0\n",
       "         ...   \n",
       "1457        0.0\n",
       "1458    65259.0\n",
       "1459    45688.0\n",
       "1460    71016.0\n",
       "1461    92855.0\n",
       "Name: annual_income, Length: 1462, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.annual_income = df.annual_income.fillna(0.0)\n",
    "df.annual_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7092925-71a0-4bd5-bc43-f6cad4ce2552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number_of_courses_viewed    0\n",
       "annual_income               0\n",
       "interaction_count           0\n",
       "lead_score                  0\n",
       "converted                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[numerical_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438c49c-9f2e-4212-ab74-f72f05381420",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the most frequent observation (mode) for the column industry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d101f1df-0034-4877-a49d-050e5a59fddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    retail\n",
       "Name: industry, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.industry.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6103b-529f-4b0e-ad62-ed87560005a3",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?\n",
    "\n",
    "    interaction_count and lead_score\n",
    "    number_of_courses_viewed and lead_score\n",
    "    number_of_courses_viewed and interaction_count\n",
    "    annual_income and interaction_count\n",
    "\n",
    "Only consider the pairs above when answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3681abb9-c9b8-46dc-9ba2-8228181ba0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[numerical_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9da4d25f-c5e6-46e8-a07f-a46f62e62681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>-0.023565</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.435914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_income</th>\n",
       "      <td>0.009770</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.053131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction_count</th>\n",
       "      <td>-0.023565</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.374573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_score</th>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted</th>\n",
       "      <td>0.435914</td>\n",
       "      <td>0.053131</td>\n",
       "      <td>0.374573</td>\n",
       "      <td>0.193673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          number_of_courses_viewed  annual_income  \\\n",
       "number_of_courses_viewed                  1.000000       0.009770   \n",
       "annual_income                             0.009770       1.000000   \n",
       "interaction_count                        -0.023565       0.027036   \n",
       "lead_score                               -0.004879       0.015610   \n",
       "converted                                 0.435914       0.053131   \n",
       "\n",
       "                          interaction_count  lead_score  converted  \n",
       "number_of_courses_viewed          -0.023565   -0.004879   0.435914  \n",
       "annual_income                      0.027036    0.015610   0.053131  \n",
       "interaction_count                  1.000000    0.009888   0.374573  \n",
       "lead_score                         0.009888    1.000000   0.193673  \n",
       "converted                          0.374573    0.193673   1.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9698bab5-95e8-4aad-b5fe-e0eb43988b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {\n",
    "    'interaction_count and lead_score': corr.loc['interaction_count','lead_score'],\n",
    "    'number_of_courses_viewed and lead_score': corr.loc['number_of_courses_viewed','lead_score'],\n",
    "    'number_of_courses_viewed and interaction_count': corr.loc['number_of_courses_viewed','interaction_count'],\n",
    "    'annual_income and interaction_count': corr.loc['annual_income','interaction_count'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb521ea6-1418-4edc-8d1e-8028e3de2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2 - Correlation pairs:\n",
      "  interaction_count and lead_score: 0.0099\n",
      "  number_of_courses_viewed and lead_score: -0.0049\n",
      "  number_of_courses_viewed and interaction_count: -0.0236\n",
      "  annual_income and interaction_count: 0.0270\n",
      "Q2 - Best pair (by abs correlation): annual_income and interaction_count value: 0.027036472404814396\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nQ2 - Correlation pairs:\")\n",
    "for k,v in pairs.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "# pick the pair with highest (absolute) correlation among these options:\n",
    "best_pair = max(pairs.items(), key=lambda kv: abs(kv[1]))\n",
    "print(\"Q2 - Best pair (by abs correlation):\", best_pair[0], \"value:\", best_pair[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addcd0e-0450-4ca8-9a2b-c7eba8df2d03",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "    Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "    Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "    Make sure that the target value converted is not in your dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d75758f-f4ad-40ec-b36a-7762ffecf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y and splits for Q3-Q6 (ensure converted is removed from features)\n",
    "df_model = df.copy()\n",
    "y = df_model['converted'].astype(int).values\n",
    "X = df_model.drop(columns=['converted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9945069-fd77-43c6-a8d3-55c11e045b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split sizes -> train: 876 val: 293 test: 293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split using sklearn train_test_split seed=42 (60/20/20)\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
    "print(\"\\nSplit sizes -> train:\", df_train.shape[0], \"val:\", df_val.shape[0], \"test:\", df_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be95ebe-daeb-4ddc-9a10-8e6bb1c52dbb",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "    Calculate the mutual information score between converted and other categorical variables in the dataset. Use the training set only.\n",
    "    Round the scores to 2 decimals using round(score, 2).\n",
    "\n",
    "Which of these variables has the biggest mutual information score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4a1ff122-28fb-4fe0-b72a-2d78b2c47f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income',\n",
       "       'employment_status', 'location', 'interaction_count', 'lead_score',\n",
       "       'converted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f023cce-e7ea-467d-bf4e-d60053c4c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Q3: mutual information between converted and categorical variables (training set only)\n",
    "cat_cols_train = [c for c in df_train.columns if df_train[c].dtype == 'object']\n",
    "print(\"Categorical columns:\", cat_cols_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d53574f-462b-4c0b-be3a-fd95d3fa1037",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# convert categories to codes for mutual_info_classif\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train_cat = \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat_cols_train\u001b[49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m s: s.astype(\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m).cat.codes)\n\u001b[32m      3\u001b[39m mi = mutual_info_classif(X_train_cat, y_train, discrete_features=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      4\u001b[39m mi_scores = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(cat_cols_train, mi))\n",
      "\u001b[31mIndexError\u001b[39m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# convert categories to codes for mutual_info_classif\n",
    "X_train_cat = X_train[cat_cols_train].apply(lambda s: s.astype('category').cat.codes)\n",
    "mi = mutual_info_classif(X_train_cat, y_train, discrete_features=True, random_state=42)\n",
    "mi_scores = dict(zip(cat_cols_train, mi))\n",
    "mi_rounded = {k: round(v,2) for k,v in mi_scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "611f368a-65d9-4d9b-ac88-64efca1161c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q3 - Mutual information (rounded):\n",
      "  lead_source: 0.04\n",
      "  industry: 0.01\n",
      "  employment_status: 0.01\n",
      "  location: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nQ3 - Mutual information (rounded):\")\n",
    "for k,v in sorted(mi_rounded.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abdb31a4-c7e5-4438-b97f-27227e4e2d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 - Candidate MIs:\n",
      "  industry: 0.01\n",
      "  location: 0.0\n",
      "  lead_source: 0.04\n",
      "  employment_status: 0.01\n",
      "Q3 - Best among given options: lead_source 0.04\n"
     ]
    }
   ],
   "source": [
    "# pick among options:\n",
    "candidates = ['industry','location','lead_source','employment_status']\n",
    "print(\"Q3 - Candidate MIs:\")\n",
    "for c in candidates:\n",
    "    print(f\"  {c}: {mi_rounded.get(c, 0.0)}\")\n",
    "best_mi = max(candidates, key=lambda c: mi_rounded.get(c, 0.0))\n",
    "print(\"Q3 - Best among given options:\", best_mi, mi_rounded.get(best_mi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b09763-1f7d-40c5-951c-cc5e46c89f9e",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "    Now let's train a logistic regression.\n",
    "    Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "    Fit the model on the training dataset.\n",
    "        To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "        model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "What accuracy did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e7e709e-f30a-4917-82cf-e6409f4a278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "val_dict = df_val[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "08912dd5-00cc-400c-941b-941d265f77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e45fc24b-db1a-4f28-9ded-2e33f0344b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X_full, feature_names\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# def prepare_ohe(dfX):\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#     num_cols_local = dfX.select_dtypes(include=[np.number]).columns.tolist()\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#     X_num = dfX[num_cols_local].values\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#     feature_names = num_cols_local + list(ohe.get_feature_names_out(cat_cols_train))\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#     return X_full, feature_names\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m X_train_ohe, feat_names = \u001b[43mprepare_ohe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m X_val_ohe, _ = prepare_ohe(df_val)\n\u001b[32m     30\u001b[39m model = LogisticRegression(solver=\u001b[33m'\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m'\u001b[39m, C=\u001b[32m1.0\u001b[39m, max_iter=\u001b[32m1000\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mprepare_ohe\u001b[39m\u001b[34m(dfX)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_cat.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m     13\u001b[39m     X_cat = X_cat.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m X_full = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m feature_names = num_cols_local + \u001b[38;5;28mlist\u001b[39m(ohe.get_feature_names_out(cat_cols_train))\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_full, feature_names\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/mlzoomcamp/lib/python3.12/site-packages/numpy/_core/shape_base.py:367\u001b[39m, in \u001b[36mhstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx.concatenate(arrs, \u001b[32m0\u001b[39m, dtype=dtype, casting=casting)\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "# Q4: logistic regression with OHE and default params per instructions\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(df_train[cat_cols_train])\n",
    "\n",
    "def prepare_ohe(dfX):\n",
    "    num_cols_local = dfX.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X_num = dfX[num_cols_local].to_numpy()\n",
    "    if X_num.ndim == 1:\n",
    "        X_num = X_num.reshape(-1, 1)\n",
    "        \n",
    "    X_cat = ohe.transform(dfX[cat_cols_train])\n",
    "    if X_cat.ndim == 1:\n",
    "        X_cat = X_cat.reshape(-1, 1)\n",
    "    \n",
    "    X_full = np.hstack([X_num, X_cat])\n",
    "    feature_names = num_cols_local + list(ohe.get_feature_names_out(cat_cols_train))\n",
    "    return X_full, feature_names\n",
    "    \n",
    "# def prepare_ohe(dfX):\n",
    "#     num_cols_local = dfX.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     X_num = dfX[num_cols_local].values\n",
    "#     X_cat = ohe.transform(dfX[cat_cols_train])\n",
    "#     X_full = np.hstack([X_num, X_cat])\n",
    "#     feature_names = num_cols_local + list(ohe.get_feature_names_out(cat_cols_train))\n",
    "#     return X_full, feature_names\n",
    "\n",
    "X_train_ohe, feat_names = prepare_ohe(df_train)\n",
    "X_val_ohe, _ = prepare_ohe(df_val)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_ohe, y_train)\n",
    "y_val_pred = model.predict(X_val_ohe)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"\\nQ4 - Validation accuracy (unrounded):\", acc_val, \"rounded:\", round(acc_val,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13c8f8-d050-40f9-a80a-e7208dbf0618",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "    Let's find the least useful feature using the feature elimination technique.\n",
    "    Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "    Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "    For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "\n",
    "Which of following feature has the smallest difference?\n",
    "\n",
    "    'industry'\n",
    "    'employment_status'\n",
    "    'lead_score'\n",
    "\n",
    "    Note: The difference doesn't have to be positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "caff4684-6866-43fe-a816-9a93eac166bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Q5: feature elimination: baseline acc minus acc when excluding each original feature\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m baseline_acc = \u001b[43macc_val\u001b[49m\n\u001b[32m      3\u001b[39m feature_diffs = {}\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m X_train.columns:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# create reduced feature matrix by removing this original feature\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'acc_val' is not defined"
     ]
    }
   ],
   "source": [
    "# Q5: feature elimination: baseline acc minus acc when excluding each original feature\n",
    "baseline_acc = acc_val\n",
    "feature_diffs = {}\n",
    "for feat in X_train.columns:\n",
    "    # create reduced feature matrix by removing this original feature\n",
    "    if feat in X_train.select_dtypes(include=[np.number]).columns:\n",
    "        # numeric: remove the exact column name from feat_names\n",
    "        keep = [i for i, name in enumerate(feat_names) if name != feat]\n",
    "    else:\n",
    "        # categorical: remove all OHE columns that start with feat + '_'\n",
    "        keep = [i for i, name in enumerate(feat_names) if not name.startswith(feat + '_')]\n",
    "    X_tr_red = X_train_ohe[:, keep]\n",
    "    X_val_red = X_val_ohe[:, keep]\n",
    "    m = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    m.fit(X_tr_red, y_train)\n",
    "    acc_red = accuracy_score(y_val, m.predict(X_val_red))\n",
    "    feature_diffs[feat] = baseline_acc - acc_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bcca9633-a117-414e-9ed6-eeaf53845c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q5 - Differences (baseline - without feature):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_diffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQ5 - Differences (baseline - without feature):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mfeature_diffs\u001b[49m.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m kv: \u001b[38;5;28mabs\u001b[39m(kv[\u001b[32m1\u001b[39m])):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m candidates_q5 = [\u001b[33m'\u001b[39m\u001b[33mindustry\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33memployment_status\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlead_score\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_diffs' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nQ5 - Differences (baseline - without feature):\")\n",
    "for k,v in sorted(feature_diffs.items(), key=lambda kv: abs(kv[1])):\n",
    "    print(f\"  {k}: {v:.6f}\")\n",
    "\n",
    "candidates_q5 = ['industry','employment_status','lead_score']\n",
    "print(\"\\nQ5 - Candidate diffs:\")\n",
    "for c in candidates_q5:\n",
    "    print(f\"  {c}: {feature_diffs.get(c)}\")\n",
    "best_q5 = min(candidates_q5, key=lambda c: abs(feature_diffs.get(c)))\n",
    "print(\"Q5 - Feature with smallest difference among candidates:\", best_q5, feature_diffs.get(best_q5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc9639-530d-4143-b24c-7a669965dc5c",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "    Now let's train a regularized logistic regression.\n",
    "    Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "    Train models using all the features as in Q4.\n",
    "    Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "\n",
    "Which of these C leads to the best accuracy on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c06cb99c-bd41-4a28-abf9-9da89d637093",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m C_results = {}\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m C \u001b[38;5;129;01min\u001b[39;00m C_values:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     m = \u001b[43mLogisticRegression\u001b[49m(solver=\u001b[33m'\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m'\u001b[39m, C=C, max_iter=\u001b[32m1000\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      6\u001b[39m     m.fit(X_train_ohe, y_train)\n\u001b[32m      7\u001b[39m     acc = accuracy_score(y_val, m.predict(X_val_ohe))\n",
      "\u001b[31mNameError\u001b[39m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Q6: test C values\n",
    "C_values = [0.01,0.1,1,10,100]\n",
    "C_results = {}\n",
    "for C in C_values:\n",
    "    m = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    m.fit(X_train_ohe, y_train)\n",
    "    acc = accuracy_score(y_val, m.predict(X_val_ohe))\n",
    "    C_results[C] = round(acc,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8db7fbd4-4f23-4179-974d-bb967df07afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q6 - Validation accuracies by C (rounded to 3 decimals):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# pick best (if ties, smallest C)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m best_acc = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mC_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m best_Cs = \u001b[38;5;28msorted\u001b[39m([c \u001b[38;5;28;01mfor\u001b[39;00m c,a \u001b[38;5;129;01min\u001b[39;00m C_results.items() \u001b[38;5;28;01mif\u001b[39;00m a==best_acc])\n\u001b[32m      7\u001b[39m best_C = best_Cs[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: max() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "print(\"\\nQ6 - Validation accuracies by C (rounded to 3 decimals):\")\n",
    "for c,v in C_results.items():\n",
    "    print(f\"  C={c}: {v}\")\n",
    "# pick best (if ties, smallest C)\n",
    "best_acc = max(C_results.values())\n",
    "best_Cs = sorted([c for c,a in C_results.items() if a==best_acc])\n",
    "best_C = best_Cs[0]\n",
    "print(\"Q6 - Best C:\", best_C, \"with val acc:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7dedf4b8-56e4-4190-aac9-df70be629db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Also print test accuracy for best model optionally\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_model = \u001b[43mLogisticRegression\u001b[49m(solver=\u001b[33m'\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m'\u001b[39m, C=best_C, max_iter=\u001b[32m1000\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      3\u001b[39m best_model.fit(X_train_ohe, y_train)\n\u001b[32m      4\u001b[39m X_test_ohe, _ = prepare_ohe(X_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Also print test accuracy for best model optionally\n",
    "best_model = LogisticRegression(solver='liblinear', C=best_C, max_iter=1000, random_state=42)\n",
    "best_model.fit(X_train_ohe, y_train)\n",
    "X_test_ohe, _ = prepare_ohe(X_test)\n",
    "print(\"Test accuracy of best C model:\", accuracy_score(y_test, best_model.predict(X_test_ohe)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b064c0b-53f3-437c-b8b1-a2bf5a7efd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows, cols: (1462, 9)\n",
      "Q1 - Mode of industry: retail\n",
      "\n",
      "Q2 - Correlation pairs:\n",
      "  interaction_count and lead_score: 0.0099\n",
      "  number_of_courses_viewed and lead_score: -0.0049\n",
      "  number_of_courses_viewed and interaction_count: -0.0236\n",
      "  annual_income and interaction_count: 0.0270\n",
      "Q2 - Best pair (by abs correlation): annual_income and interaction_count value: 0.027036472404814396\n",
      "\n",
      "Split sizes -> train: 876 val: 293 test: 293\n",
      "\n",
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "\n",
      "Q3 - Mutual information (rounded):\n",
      "  lead_source: 0.04\n",
      "  industry: 0.01\n",
      "  employment_status: 0.01\n",
      "  location: 0.0\n",
      "Q3 - Candidate MIs:\n",
      "  industry: 0.01\n",
      "  location: 0.0\n",
      "  lead_source: 0.04\n",
      "  employment_status: 0.01\n",
      "Q3 - Best among given options: lead_source 0.04\n",
      "\n",
      "Q4 - Validation accuracy (unrounded): 0.6996587030716723 rounded: 0.7\n",
      "\n",
      "Q5 - Differences (baseline - without feature):\n",
      "  industry: 0.000000\n",
      "  employment_status: 0.003413\n",
      "  lead_source: -0.003413\n",
      "  lead_score: -0.006826\n",
      "  location: -0.010239\n",
      "  number_of_courses_viewed: 0.143345\n",
      "  interaction_count: 0.143345\n",
      "  annual_income: -0.153584\n",
      "\n",
      "Q5 - Candidate diffs:\n",
      "  industry: 0.0\n",
      "  employment_status: 0.0034129692832763903\n",
      "  lead_score: -0.0068259385665528916\n",
      "Q5 - Feature with smallest difference among candidates: industry 0.0\n",
      "\n",
      "Q6 - Validation accuracies by C (rounded to 3 decimals):\n",
      "  C=0.01: 0.7\n",
      "  C=0.1: 0.7\n",
      "  C=1: 0.7\n",
      "  C=10: 0.7\n",
      "  C=100: 0.7\n",
      "Q6 - Best C: 0.01 with val acc: 0.7\n",
      "Test accuracy of best C model: 0.7303754266211604\n"
     ]
    }
   ],
   "source": [
    "# Save as lead_homework.py and run with: python lead_homework.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1) Load data\n",
    "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(\"Rows, cols:\", df.shape)\n",
    "\n",
    "# 2) Data prep: categorical NaN -> 'NA', numeric NaN -> 0.0\n",
    "df_prep = df.copy()\n",
    "cat_cols = df_prep.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = df_prep.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "for c in cat_cols:\n",
    "    df_prep[c] = df_prep[c].fillna('NA')\n",
    "for c in num_cols:\n",
    "    df_prep[c] = df_prep[c].fillna(0.0)\n",
    "\n",
    "# Q1: mode for industry\n",
    "mode_industry = df_prep['industry'].mode().iloc[0]\n",
    "print(\"Q1 - Mode of industry:\", mode_industry)\n",
    "\n",
    "# Q2: correlation among numeric features -> check specified pairs\n",
    "corr = df_prep[num_cols].corr()\n",
    "pairs = {\n",
    "    'interaction_count and lead_score': corr.loc['interaction_count','lead_score'],\n",
    "    'number_of_courses_viewed and lead_score': corr.loc['number_of_courses_viewed','lead_score'],\n",
    "    'number_of_courses_viewed and interaction_count': corr.loc['number_of_courses_viewed','interaction_count'],\n",
    "    'annual_income and interaction_count': corr.loc['annual_income','interaction_count'],\n",
    "}\n",
    "print(\"\\nQ2 - Correlation pairs:\")\n",
    "for k,v in pairs.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "# pick the pair with highest (absolute) correlation among these options:\n",
    "best_pair = max(pairs.items(), key=lambda kv: abs(kv[1]))\n",
    "print(\"Q2 - Best pair (by abs correlation):\", best_pair[0], \"value:\", best_pair[1])\n",
    "\n",
    "# Prepare X, y and splits for Q3-Q6 (ensure converted is removed from features)\n",
    "df_model = df_prep.copy()\n",
    "y = df_model['converted'].astype(int).values\n",
    "X = df_model.drop(columns=['converted'])\n",
    "\n",
    "# Split using sklearn train_test_split seed=42 (60/20/20)\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)\n",
    "print(\"\\nSplit sizes -> train:\", X_train.shape[0], \"val:\", X_val.shape[0], \"test:\", X_test.shape[0])\n",
    "\n",
    "# Q3: mutual information between converted and categorical variables (training set only)\n",
    "cat_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nCategorical columns:\", cat_cols_train)\n",
    "# convert categories to codes for mutual_info_classif\n",
    "X_train_cat = X_train[cat_cols_train].apply(lambda s: s.astype('category').cat.codes)\n",
    "mi = mutual_info_classif(X_train_cat, y_train, discrete_features=True, random_state=42)\n",
    "mi_scores = dict(zip(cat_cols_train, mi))\n",
    "mi_rounded = {k: round(v,2) for k,v in mi_scores.items()}\n",
    "print(\"\\nQ3 - Mutual information (rounded):\")\n",
    "for k,v in sorted(mi_rounded.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    print(f\"  {k}: {v}\")\n",
    "# pick among options:\n",
    "candidates = ['industry','location','lead_source','employment_status']\n",
    "print(\"Q3 - Candidate MIs:\")\n",
    "for c in candidates:\n",
    "    print(f\"  {c}: {mi_rounded.get(c, 0.0)}\")\n",
    "best_mi = max(candidates, key=lambda c: mi_rounded.get(c, 0.0))\n",
    "print(\"Q3 - Best among given options:\", best_mi, mi_rounded.get(best_mi))\n",
    "\n",
    "# Q4: logistic regression with OHE and default params per instructions\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X_train[cat_cols_train])\n",
    "\n",
    "def prepare_ohe(dfX):\n",
    "    num_cols_local = dfX.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X_num = dfX[num_cols_local].to_numpy()\n",
    "    if X_num.ndim == 1:\n",
    "        X_num = X_num.reshape(-1, 1)\n",
    "\n",
    "    # Handle categorical features\n",
    "    X_cat = ohe.transform(dfX[cat_cols_train])\n",
    "    if isinstance(X_cat, np.ndarray) and X_cat.ndim == 1:\n",
    "        X_cat = X_cat.reshape(-1, 1)\n",
    "    elif not isinstance(X_cat, np.ndarray):\n",
    "        X_cat = X_cat.toarray()  # For sparse output from OneHotEncoder\n",
    "\n",
    "    # Combine numeric + categorical\n",
    "    X_full = np.hstack([X_num, X_cat])\n",
    "    feature_names = num_cols_local + list(ohe.get_feature_names_out(cat_cols_train))\n",
    "    return X_full, feature_names\n",
    "\n",
    "X_train_ohe, feat_names = prepare_ohe(X_train)\n",
    "X_val_ohe, _ = prepare_ohe(X_val)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_ohe, y_train)\n",
    "y_val_pred = model.predict(X_val_ohe)\n",
    "acc_val = accuracy_score(y_val, y_val_pred)\n",
    "print(\"\\nQ4 - Validation accuracy (unrounded):\", acc_val, \"rounded:\", round(acc_val,2))\n",
    "\n",
    "# Q5: feature elimination: baseline acc minus acc when excluding each original feature\n",
    "baseline_acc = acc_val\n",
    "feature_diffs = {}\n",
    "for feat in X_train.columns:\n",
    "    # create reduced feature matrix by removing this original feature\n",
    "    if feat in X_train.select_dtypes(include=[np.number]).columns:\n",
    "        # numeric: remove the exact column name from feat_names\n",
    "        keep = [i for i, name in enumerate(feat_names) if name != feat]\n",
    "    else:\n",
    "        # categorical: remove all OHE columns that start with feat + '_'\n",
    "        keep = [i for i, name in enumerate(feat_names) if not name.startswith(feat + '_')]\n",
    "    X_tr_red = X_train_ohe[:, keep]\n",
    "    X_val_red = X_val_ohe[:, keep]\n",
    "    m = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    m.fit(X_tr_red, y_train)\n",
    "    acc_red = accuracy_score(y_val, m.predict(X_val_red))\n",
    "    feature_diffs[feat] = baseline_acc - acc_red\n",
    "\n",
    "print(\"\\nQ5 - Differences (baseline - without feature):\")\n",
    "for k,v in sorted(feature_diffs.items(), key=lambda kv: abs(kv[1])):\n",
    "    print(f\"  {k}: {v:.6f}\")\n",
    "\n",
    "candidates_q5 = ['industry','employment_status','lead_score']\n",
    "print(\"\\nQ5 - Candidate diffs:\")\n",
    "for c in candidates_q5:\n",
    "    print(f\"  {c}: {feature_diffs.get(c)}\")\n",
    "best_q5 = min(candidates_q5, key=lambda c: abs(feature_diffs.get(c)))\n",
    "print(\"Q5 - Feature with smallest difference among candidates:\", best_q5, feature_diffs.get(best_q5))\n",
    "\n",
    "# Q6: test C values\n",
    "C_values = [0.01,0.1,1,10,100]\n",
    "C_results = {}\n",
    "for C in C_values:\n",
    "    m = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    m.fit(X_train_ohe, y_train)\n",
    "    acc = accuracy_score(y_val, m.predict(X_val_ohe))\n",
    "    C_results[C] = round(acc,3)\n",
    "\n",
    "print(\"\\nQ6 - Validation accuracies by C (rounded to 3 decimals):\")\n",
    "for c,v in C_results.items():\n",
    "    print(f\"  C={c}: {v}\")\n",
    "# pick best (if ties, smallest C)\n",
    "best_acc = max(C_results.values())\n",
    "best_Cs = sorted([c for c,a in C_results.items() if a==best_acc])\n",
    "best_C = best_Cs[0]\n",
    "print(\"Q6 - Best C:\", best_C, \"with val acc:\", best_acc)\n",
    "\n",
    "# Also print test accuracy for best model optionally\n",
    "best_model = LogisticRegression(solver='liblinear', C=best_C, max_iter=1000, random_state=42)\n",
    "best_model.fit(X_train_ohe, y_train)\n",
    "X_test_ohe, _ = prepare_ohe(X_test)\n",
    "print(\"Test accuracy of best C model:\", accuracy_score(y_test, best_model.predict(X_test_ohe)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46664901-4445-4b6f-9330-045d27bc6a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
